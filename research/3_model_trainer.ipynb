{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env `nlp_mlops`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Marina\\\\Desktop\\\\ML Operations\\\\0 - KrishNaik Course\\\\21_end_to_end_nlp_project_with_huggingface_and_transformers\\\\my_project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos fazer isso, criando a classe `model_trainer` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Params.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criado para nos auxiliar no treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Config entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainerConfig():\n",
    "  root_dir: Path\n",
    "  data_path: Path \n",
    "  model_ckpt: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainerParams():\n",
    "  num_train_epochs: int\n",
    "  warmup_steps: int\n",
    "  per_device_train_batch_size: int\n",
    "  weight_decay: float\n",
    "  logging_steps: int\n",
    "  evaluation_strategy: str\n",
    "  eval_steps: int\n",
    "  save_steps: int\n",
    "  gradient_accumulation_steps: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar umas constanstes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.textSummarizer.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from src.textSummarizer.utils.common import read_yaml, create_directories\n",
    "from typing import Tuple\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                config_path= CONFIG_FILE_PATH,\n",
    "                params_path= PARAMS_FILE_PATH ):\n",
    "        \n",
    "        self.configurations = read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "\n",
    "        create_directories([self.configurations.artifacts_root]) # cria o /artifacts\n",
    "\n",
    "\n",
    "    def get_model_trainer_config(self)-> Tuple[ModelTrainerConfig, ModelTrainerParams]:\n",
    "        \n",
    "        model_trainer_config = self.configurations.model_trainer\n",
    "        model_trainer_params = self.params[\"TrainingArguments\"]\n",
    "    \n",
    "        \n",
    "        create_directories([model_trainer_config.root_dir]) # cria o /artifacts/model_trainer\n",
    "\n",
    "        return model_trainer_config, model_trainer_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2024-11-13 11:05:20,684 ] - 28 summarizerlogger - INFO - yaml file: config\\config.yaml loaded successfully\n",
      "[ 2024-11-13 11:05:20,701 ] - 28 summarizerlogger - INFO - yaml file: params.yaml loaded successfully\n",
      "[ 2024-11-13 11:05:20,705 ] - 46 summarizerlogger - INFO - created directory at: artifacts\n",
      "[ 2024-11-13 11:05:20,708 ] - 46 summarizerlogger - INFO - created directory at: artifacts/model_trainer\n",
      "{'root_dir': 'artifacts/model_trainer', 'data_path': 'artifacts/data_transformation/samsum_dataset', 'model_ckpt': 'google/pegasus-cnn_dailymail'}\n",
      "{'num_train_epochs': 1, 'warmup_steps': 500, 'per_device_train_batch_size': 1, 'weight_decay': 0.01, 'logging_steps': 10, 'evaluation_strategy': 'steps', 'eval_steps': 500, 'save_steps': 1000000, 'gradient_accumulation_steps': 16}\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "model_trainer_config, model_trainer_params = config.get_model_trainer_config()\n",
    "print(model_trainer_config)\n",
    "print(model_trainer_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Update the components- Data Ingestion,Data Transformation, Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2024-11-13 11:13:55,072 ] - 54 datasets - INFO - PyTorch version 2.1.2 available.\n",
      "[ 2024-11-13 11:13:55,077 ] - 112 datasets - INFO - TensorFlow version 2.13.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import torch    \n",
    "# from datasets import load_from_disk\n",
    "\n",
    "#keras==2.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets\n",
    "from datasets import load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.textSummarizer.logging import logger\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self,\n",
    "                 model_trainer_config: ModelTrainerConfig, \n",
    "                 model_trainer_params: ModelTrainerParams):\n",
    "        \"\"\"\n",
    "        Initializes the ModelTrainer class with configuration details.\n",
    "\n",
    "        Args:   \n",
    "            model_trainer_config (ModelTrainerConfig): Configuration object containing the needed paths to perform the model training.\n",
    "            model_trainer_params (ModelTrainerParams): Params object containing the needed params to perform the model training.\n",
    "        \"\"\"\n",
    "        self.config = model_trainer_config\n",
    "        self.params = model_trainer_params\n",
    "        logger.info(\"ModelTrainer initialized with configuration and parameters.\")\n",
    "\n",
    "    def train(self):\n",
    "        # Determine the device to use\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "\n",
    "        # Load tokenizer and model\n",
    "        logger.info(\"Loading tokenizer and model.\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n",
    "        logger.info(\"Tokenizer and model loaded successfully.\")\n",
    "\n",
    "        # Load the dataset\n",
    "        logger.info(f\"Loading dataset from {self.config.data_path}.\")\n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "        logger.info(\"Dataset loaded successfully.\")\n",
    "        \n",
    "\n",
    "        # Set up training arguments\n",
    "        logger.info(\"Setting up training arguments.\")\n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir,\n",
    "            **self.params\n",
    "        )\n",
    "        logger.info(\"Training arguments set.\")\n",
    "\n",
    "        # Initialize the trainer\n",
    "        logger.info(\"Initializing Trainer.\")\n",
    "        trainer = Trainer(\n",
    "            model=model_pegasus,\n",
    "            args=trainer_args,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=seq2seq_data_collator,\n",
    "            train_dataset=dataset_samsum_pt[\"test\"],\n",
    "            eval_dataset=dataset_samsum_pt[\"validation\"]\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        logger.info(\"Starting training process.\")\n",
    "        trainer.train()\n",
    "        logger.info(\"Training completed.\")\n",
    "\n",
    "        # Save model and tokenizer\n",
    "        logger.info(\"Saving the trained model and tokenizer.\")\n",
    "        model_pegasus.save_pretrained(os.path.join(self.config.root_dir, \"pegasus-samsum-model\"))\n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir, \"tokenizer\"))\n",
    "        logger.info(\"Model and tokenizer saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2024-11-13 11:14:05,091 ] - 28 summarizerlogger - INFO - yaml file: config\\config.yaml loaded successfully\n",
      "[ 2024-11-13 11:14:05,142 ] - 28 summarizerlogger - INFO - yaml file: params.yaml loaded successfully\n",
      "[ 2024-11-13 11:14:05,147 ] - 46 summarizerlogger - INFO - created directory at: artifacts\n",
      "[ 2024-11-13 11:14:05,190 ] - 46 summarizerlogger - INFO - created directory at: artifacts/model_trainer\n",
      "[ 2024-11-13 11:14:05,191 ] - 16 summarizerlogger - INFO - ModelTrainer initialized with configuration and parameters.\n",
      "[ 2024-11-13 11:14:05,193 ] - 21 summarizerlogger - INFO - Using device: cpu\n",
      "[ 2024-11-13 11:14:05,196 ] - 24 summarizerlogger - INFO - Loading tokenizer and model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marina\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Marina\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2024-11-13 11:15:21,421 ] - 28 summarizerlogger - INFO - Tokenizer and model loaded successfully.\n",
      "[ 2024-11-13 11:15:21,456 ] - 31 summarizerlogger - INFO - Loading dataset from artifacts/data_transformation/samsum_dataset.\n",
      "[ 2024-11-13 11:15:23,304 ] - 33 summarizerlogger - INFO - Dataset loaded successfully.\n",
      "[ 2024-11-13 11:15:23,312 ] - 37 summarizerlogger - INFO - Setting up training arguments.\n",
      "[ 2024-11-13 11:15:23,760 ] - 42 summarizerlogger - INFO - Training arguments set.\n",
      "[ 2024-11-13 11:15:23,761 ] - 45 summarizerlogger - INFO - Initializing Trainer.\n",
      "[ 2024-11-13 11:15:52,063 ] - 56 summarizerlogger - INFO - Starting training process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marina\\AppData\\Roaming\\Python\\Python311\\site-packages\\accelerate\\accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'TypeAliasType' could not be imported from 'c:\\Users\\Marina\\.conda\\envs\\nlp_mlops\\Lib\\site-packages\\typing_extensions.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "configuration_manager_obj = ConfigurationManager()\n",
    "model_trainer_config, model_trainer_params = configuration_manager_obj.get_model_trainer_config()\n",
    "\n",
    "model_trainer_obj = ModelTrainer(model_trainer_config, model_trainer_params)\n",
    "model_trainer_obj.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emulei o treinamento usando o google colab e baixnado o modelo e o tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modularizar o Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `3.` vai para `src\\textSummarizer\\entity\\__init__.py`\n",
    "\n",
    "O `4.` vai para `src\\textSummarizer\\config\\configuration.py`\n",
    "\n",
    "O `5.` vai para `src\\textSummarizer\\components\\data_transformation.py`\n",
    "\n",
    "Modularizamos criando uma pipeline (classe) em `stage_3_model_trainer_pipeline.py`, com o que usamos para rodar o código\n",
    "\n",
    "Jogar a Pipeline para `main.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
